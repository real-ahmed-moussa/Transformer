# Transformer Model: Encoderâ€“Decoder Architecture in PyTorch

<p align="center">
  <img src="imgs/TM.png" alt="TM" width="300">
</p>

> Implementation of the Transformer architecture from scratch using PyTorch. This project demonstrates the key building blocks of Transformersâ€”positional encoding, multi-head attention, encoder and decoder layersâ€”without relying on high-level libraries like Hugging Face.

---

## ğŸ“– Overview
This project provides a clear, modular implementation of the Transformer model in PyTorch. It can serve as an **educational reference** for understanding sequence-to-sequence architectures, attention mechanisms, and model training workflows.

The notebook includes detailed class definitions for:  
- Positional Encoding
- Multi-Head Attention
- Feedforward Layers
- Encoder & Decoder Blocks
- Full Encoderâ€“Decoder Transformer

---

## ğŸš€ Features
âœ… **Pure PyTorch Implementation:** No reliance on pre-built Transformer classes.  
âœ… **Educational Design:** Each subcomponent (e.g., positional encoding, attention) is implemented and explained.  
âœ… **Customizable Architecture:** Adjust number of layers, heads, and embedding sizes.  
âœ… **Training Loop Ready:** Compatible with PyTorch DataLoader and optimizers (Adam).  


## ğŸ¢ Applications
This implementation can be used to:  
- Build a translation model (e.g., Englishâ€“Spanish).
- Experiment with sequence modeling tasks (summarization, text generation).
- Serve as a learning tool for students and practitioners studying Transformers.

---

## âš™ï¸ Tech Stack
| Technology   | Purpose                                                    |
| -----------  | ---------------------------------------------------------- |
| `PyTorch`    | Core implementation (attention, encoder/decoder, training) |
| `Lightning`  | Training orchestration (optional, via PyTorch Lightning)   |

---

## Original Transformer Architecture
Figure is from the original ***Attention Is All You Need*** paper (see the references).
<p align="center">
  <img src="imgs/Original_Transformer_Arch.png" alt="Original Transformer Architecture" width="500">
</p>

---

## ğŸ“‚ Project Structure
<pre>
ğŸ“¦ Transformer Model - Encoderâ€“Decoder Architecture in PyTorch
 â”£ ğŸ“‚ imgs
 â”£ ğŸ“œ Transformer_Model.ipynb          # Jupyter Notebook with full implementation
 â”— README.md               
</pre>

---

## ğŸ› ï¸ Installation
1ï¸âƒ£ **Clone the Repository**
<pre>
git clone https://github.com/ahmedmoussa/Transformer.git
cd Transformer
</pre>

2ï¸âƒ£ **Install Dependencies**
<pre>
pip install torch lightning
</pre>

3ï¸âƒ£ **Open Notebook**
<pre>
jupyter notebook Transformer_Model.ipynb
</pre>

---

## ğŸ“Š Results
- **Multi-Head Attention** successfully captures token-level dependencies across sequences.
- **Encoderâ€“Decoder stacking** allows contextual learning and sequence generation.
- Model achieves **stable training** using Adam optimizer and LayerNorm.
> âš¡ For demonstration purposes, toy datasets (e.g., random sequences) can be used. Real NLP tasks (translation, summarization) can be plugged in with minimal modifications.

---

## ğŸ“š References
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems (NeurIPS).

---

## ğŸ“ License
This project is shared for portfolio purposes only and may not be used for commercial purposes without permission.

This project is licensed under the **MIT License**.  
Â© 2025 **Dr. Ahmed Moussa**
